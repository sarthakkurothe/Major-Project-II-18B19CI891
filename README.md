
# ðŸ§  Integrative Multimodal Analysis for Schizophrenia Cause Identification

## ðŸ“ Project Description

Schizophrenia is a complex mental disorder affecting perception, cognition, and behavior. Despite extensive research, its underlying causes remain unclear due to the limitations of unimodal analyses. This project addresses that gap by implementing a **multimodal deep learning framework** integrating **video, audio, and textual** data from patient interviews to identify potential markers and causes of schizophrenia. Through advanced techniques like **transformers, multimodal fusion (mGMU), and NLP**, we aim to improve diagnosis accuracy and enable personalized treatment approaches.

## ðŸ“‚ Folder Structure

```
.
â”œâ”€â”€ data/                      # Raw and preprocessed data (video, audio, text)
â”œâ”€â”€ models/                   # Trained models and checkpoints
â”œâ”€â”€ preprocessing/            # Scripts for preprocessing all modalities
â”‚   â”œâ”€â”€ video_preprocessing.py
â”‚   â”œâ”€â”€ audio_preprocessing.py
â”‚   â””â”€â”€ text_preprocessing.py
â”œâ”€â”€ multimodal_model/         # Transformer model and mGMU integration
â”œâ”€â”€ utils/                    # Helper functions (synchronization, augmentation, etc.)
â”œâ”€â”€ results/                  # Outputs, evaluation metrics, and visualizations
â”œâ”€â”€ docs/                     # Documentation and reference papers
â”œâ”€â”€ README.md                 # Project overview (this file)
â””â”€â”€ requirements.txt          # Required dependencies
```

## ðŸ’¡ Source Code Overview

- **Video Processing**: Uses OpenCV and Mediapipe to extract facial gestures, gaze, and expressions.
- **Audio Processing**: Uses Librosa to extract MFCCs, pitch, pause patterns; includes noise reduction.
- **Text Processing**: NLP-based techniques for sentiment analysis, coherence, and keyword extraction.
- **Multimodal Integration**: A unified dataset is created using timestamp-based alignment.
- **Model Architecture**: A transformer-based model integrated with a **Gated Multimodal Unit (mGMU)** processes the synchronized data for classification and pattern recognition.

## ðŸ“š Documentation

- **Model Architecture**: See `/docs/multimodal_architecture.md`
- **Data Pipeline**: Detailed in `/docs/preprocessing_pipeline.md`
- **References**: Listed in the final section of this README and available in `/docs/references.bib`

## âš™ï¸ Installation Instructions

```bash
# Clone the repository
git clone https://github.com/your-username/schizophrenia-multimodal-analysis.git
cd schizophrenia-multimodal-analysis

# Set up a virtual environment
python -m venv venv
source venv/bin/activate  # For Linux/macOS
venv\Scripts\activate     # For Windows

# Install dependencies
pip install -r requirements.txt
```

## ðŸ“¹ 5-Minute Video Demonstration

ðŸŽ¥ [Watch Demo Video](https://your-video-link.com)  
> This video walks through the project's goals, methodology, demo results, and key findings. It includes a high-level overview of the transformer-based multimodal framework and showcases the data pipeline, visual outputs, and model predictions.

## ðŸ§ª Additional Features

### âœ… Project Board & Issues
- We used GitHub Projects to plan milestones and track progress.
- All bugs, enhancements, and discussions are tracked under [GitHub Issues](https://github.com/your-repo/issues).

### ðŸ”¬ Unit Tests & Results
- Basic unit tests are provided for preprocessing modules using `pytest`.
- Evaluation results (accuracy, precision, recall, F1-score) are included in the `/results` directory.

## ðŸ”® Future Work

- Train the model on extended datasets including EEG/fMRI.
- Apply one-shot learning for better generalization on small datasets.
- Deploy as a clinical support tool via a web/mobile app for real-time diagnostics.

## ðŸ‘¥ Team & Contributions

- **Sarthak Kurothe** (211420) â€“ Audio & Text Processing, Model Implementation 
- **Shivansh Kushwaha** (211308) â€“ Video Processing, Experimental Evaluation

**Supervisor**: Dr. Kushal Kanwar (Assistant Professor, CSE, JUIT)

## ðŸ“– References

A detailed list of all academic references used in the project is available in the [Presentation Appendix](./docs/references.bib) and includes works such as:

1. Premananth et al., *Interspeech*, 2024  
2. GÃ¶ker, SpringerLink, 2023  
3. Chuang et al., *IEEE Transactions on Neural Systems*, 2023  
... *(and more, see presentation)*
