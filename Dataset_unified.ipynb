{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV2RZm1O4fkE",
        "outputId": "48299488-b12e-4fa8-cac1-4af0984fd392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "OcOTilyR3yei",
        "outputId": "6cf49074-7391-4c13-c454-5fb7d42e2cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video analysis file not found: /content/drive/MyDrive/Datasetr/processed_face_postures/Interview_1_face_postures.json\n",
            "Skipping interview: Interview_1\n",
            "Video analysis file not found: /content/drive/MyDrive/Datasetr/processed_face_postures/Interview_2_face_postures.json\n",
            "Skipping interview: Interview_2\n",
            "Video analysis file not found: /content/drive/MyDrive/Datasetr/processed_face_postures/Interview_3_face_postures.json\n",
            "Skipping interview: Interview_3\n",
            "Video analysis file not found: /content/drive/MyDrive/Datasetr/processed_face_postures/Interview_4_face_postures.json\n",
            "Skipping interview: Interview_4\n",
            "Video analysis file not found: /content/drive/MyDrive/Datasetr/processed_face_postures/Interview_5_face_postures.json\n",
            "Skipping interview: Interview_5\n",
            "Video analysis file not found: /content/drive/MyDrive/Datasetr/processed_face_postures/Interview_6_face_postures.json\n",
            "Skipping interview: Interview_6\n",
            "Video analysis file not found: /content/drive/MyDrive/Datasetr/processed_face_postures/Interview_7_face_postures.json\n",
            "Skipping interview: Interview_7\n",
            "Video analysis file not found: /content/drive/MyDrive/Datasetr/processed_face_postures/Interview_8_face_postures.json\n",
            "Skipping interview: Interview_8\n",
            "Video analysis file not found: /content/drive/MyDrive/Datasetr/processed_face_postures/Interview_9_face_postures.json\n",
            "Skipping interview: Interview_9\n",
            "Video analysis file not found: /content/drive/MyDrive/Datasetr/processed_face_postures/Interview_10_face_postures.json\n",
            "Skipping interview: Interview_10\n",
            "Video analysis file not found: /content/drive/MyDrive/Datasetr/processed_face_postures/Interview_11_face_postures.json\n",
            "Skipping interview: Interview_11\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Datasetr/unified_dataset.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-96e74e1601eb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Save unified data to a JSON file (you can also save it as CSV if you prefer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Datasetr/unified_dataset.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munified_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Datasetr/unified_dataset.json'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Define your data directories\n",
        "video_segments_dir = \"/content/drive/MyDrive/Datasetr/Dataset/Videos/\"\n",
        "processed_video_dir = \"/content/drive/MyDrive/Datasetr/processed_face_postures/\"\n",
        "processed_audio_dir = \"/content/drive/MyDrive/Datasetr/processed_audio/\"\n",
        "processed_transcripts_dir = \"/content/drive/MyDrive/Datasetr/processed_transcripts/\"\n",
        "\n",
        "# Function to read preprocessed video frame features\n",
        "def read_video_features(video_id):\n",
        "    \"\"\"\n",
        "    Reads video features from the processed video JSON file.\n",
        "\n",
        "    Args:\n",
        "        video_id (int): The interview ID (e.g., 10 for Interview_10).\n",
        "\n",
        "    Returns:\n",
        "        list: List of features extracted from the video.\n",
        "    \"\"\"\n",
        "    video_file_path = os.path.join(processed_video_dir, f\"Interview_{video_id}_face_postures.json\")\n",
        "    if not os.path.exists(video_file_path):\n",
        "        raise FileNotFoundError(f\"Video analysis file not found: {video_file_path}\")\n",
        "\n",
        "    with open(video_file_path, \"r\") as f:\n",
        "        video_features = json.load(f)\n",
        "    return video_features\n",
        "\n",
        "# Function to read preprocessed audio features\n",
        "def read_audio_features(interview_id):\n",
        "    \"\"\"\n",
        "    Reads audio features from the processed audio JSON file.\n",
        "    \"\"\"\n",
        "    audio_file_path = os.path.join(processed_audio_dir, f\"Interview_{interview_id}.json\")\n",
        "    if not os.path.exists(audio_file_path):\n",
        "        raise FileNotFoundError(f\"Audio analysis file not found: {audio_file_path}\")\n",
        "\n",
        "    with open(audio_file_path, \"r\") as f:\n",
        "        audio_features = json.load(f)\n",
        "    return audio_features\n",
        "\n",
        "# Function to read preprocessed transcript features\n",
        "def read_transcript_features(interview_id):\n",
        "    \"\"\"\n",
        "    Reads transcript features from the processed transcript JSON file.\n",
        "    \"\"\"\n",
        "    transcript_file_path = os.path.join(processed_transcripts_dir, f\"Interview_{interview_id}.json\")\n",
        "    if not os.path.exists(transcript_file_path):\n",
        "        raise FileNotFoundError(f\"Transcript analysis file not found: {transcript_file_path}\")\n",
        "\n",
        "    with open(transcript_file_path, \"r\") as f:\n",
        "        transcript_features = json.load(f)\n",
        "    return transcript_features\n",
        "\n",
        "# Function to create a unified data structure\n",
        "def create_unified_structure():\n",
        "\n",
        "    unified_data = []\n",
        "\n",
        "    # Iterate through each interview\n",
        "    for i in range(1, 12):  # Process all interviews (1 to 11)\n",
        "        interview_id = i  # Numeric ID for the interview\n",
        "\n",
        "        try:\n",
        "            # Get preprocessed features\n",
        "            video_features = read_video_features(interview_id)\n",
        "            audio_features = read_audio_features(interview_id)\n",
        "            transcript_features = read_transcript_features(interview_id)\n",
        "\n",
        "            # Create unified data structure for the current interview\n",
        "            interview_data = {\n",
        "                \"interview_id\": f\"Interview_{interview_id}\",\n",
        "                \"video_features\": video_features,  # Frame-level features\n",
        "                \"audio_features\": audio_features,  # Audio-level features\n",
        "                \"transcript_features\": transcript_features,  # Linguistic features\n",
        "            }\n",
        "\n",
        "            # Append to the overall dataset\n",
        "            unified_data.append(interview_data)\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            print(e)\n",
        "            print(f\"Skipping interview: Interview_{interview_id}\")\n",
        "\n",
        "    return unified_data\n",
        "\n",
        "# Generate unified data\n",
        "unified_dataset = create_unified_structure()\n",
        "\n",
        "# Save unified data to a JSON file (you can also save it as CSV if you prefer)\n",
        "output_path = \"/content/drive/MyDrive/Datasetr/unified_dataset.json\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(unified_dataset, f, indent=4)\n",
        "\n",
        "print(f\"Unified dataset created and saved successfully at {output_path}.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}